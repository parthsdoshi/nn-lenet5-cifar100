{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lenet5",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthsdoshi/nn-lenet5-cifar100/blob/master/lenet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YN6m9zrUIu6j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision as tv\n",
        "import torch.optim as O"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8QyjLvKjJbc9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-WqOFoS-KaUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7630b981-d183-4460-dff7-3bbf50f1f40b"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uBkyGzeNDHrp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# flattens so we can go from conv layers to linear layers\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4KuuFoDzKmSQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def createLenet5(in_channels=3, init_padding=(0, 0), classes=10, activation=nn.ReLU):\n",
        "    lenet5 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 6, kernel_size=(5, 5), padding=init_padding),\n",
        "        activation(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
        "        nn.Conv2d(6, 16, kernel_size=(5, 5)),\n",
        "        activation(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
        "        Flatten(),\n",
        "        nn.Linear(16*5*5, 120),\n",
        "        activation(),\n",
        "        nn.Linear(120, 84),\n",
        "        activation(),\n",
        "        nn.Linear(84, classes)\n",
        "    )\n",
        "\n",
        "    return lenet5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNCs4O0uMDFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tv.datasets.MNIST\n",
        "cifar100 = tv.datasets.CIFAR100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EoDMU5nBNgsw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_batch = 1\n",
        "test_batch = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALhzyIZHMNYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_train_loader = torch.utils.data.DataLoader(mnist('./mnist/train', train=True, download=True,\n",
        "                                                           transform=tv.transforms.Compose([\n",
        "                                                               tv.transforms.ToTensor()\n",
        "                                                           ])), batch_size=train_batch, shuffle=True)\n",
        "mnist_train_test_loader = torch.utils.data.DataLoader(mnist('./mnist/train', train=True, download=True,\n",
        "                                                           transform=tv.transforms.Compose([\n",
        "                                                               tv.transforms.ToTensor()\n",
        "                                                           ])), batch_size=test_batch, shuffle=True)\n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist('./mnist/test', train=False, download=True,\n",
        "                                                           transform=tv.transforms.Compose([\n",
        "                                                               tv.transforms.ToTensor()\n",
        "                                                           ])), batch_size=test_batch, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8j-aLReN8D9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7426514a-ebf3-4ccd-f30e-0bcda46d5365"
      },
      "cell_type": "code",
      "source": [
        "cifar100_train_loader = torch.utils.data.DataLoader(cifar100('./cifar100/train', train=True, download=True,\n",
        "                                                           transform=tv.transforms.Compose([\n",
        "                                                               tv.transforms.ToTensor()\n",
        "                                                           ])), batch_size=train_batch, shuffle=True)\n",
        "cifar100_train_test_loader = torch.utils.data.DataLoader(cifar100('./cifar100/train', train=True, download=True,\n",
        "                                                           transform=tv.transforms.Compose([\n",
        "                                                               tv.transforms.ToTensor()\n",
        "                                                           ])), batch_size=test_batch, shuffle=True)\n",
        "cifar100_test_loader = torch.utils.data.DataLoader(cifar100('./cifar100/test', train=False, download=True,\n",
        "                                                           transform=tv.transforms.Compose([\n",
        "                                                               tv.transforms.ToTensor()\n",
        "                                                           ])), batch_size=test_batch, shuffle=True)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yI6-guA5ORQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "695b1676-b0e5-4616-cd08-c76b081e8863"
      },
      "cell_type": "code",
      "source": [
        "# looks like our datasets are stored in /content\n",
        "!cat /content/sample_data/README.md"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This directory includes a few sample datasets to get you started.\n",
            "\n",
            "* `california_housing_data*.csv` is California housing data from the 1990 US\n",
            "  Census; more information is available at:\n",
            "  https://developers.google.com/machine-learning/crash-course/california-housing-data-description\n",
            "\n",
            "* `mnist_*.csv` is a small sample of the [MNIST\n",
            "  database](https://en.wikipedia.org/wiki/MNIST_database), which is described\n",
            "  at: http://yann.lecun.com/exdb/mnist/\n",
            "\n",
            "* `anscombe.json` contains a copy of [Anscombe's\n",
            " quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);\n",
            "  it was originally described in\n",
            "\n",
            "      Anscombe, F. J. (1973). 'Graphs in Statistical Analysis'. American Statistician. 27 (1): 17-21. JSTOR 2682899.\n",
            "\n",
            "  and our copy was prepared by the [vega_datasets library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FJ4sMLouPQYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6d3f6035-e888-4357-c332-04e2f6ecab1a"
      },
      "cell_type": "code",
      "source": [
        "print(\"mnist\")\n",
        "print(f\"train size: {len(mnist_train_loader) * train_batch}\\ttest size: {len(mnist_test_loader) * test_batch}\")\n",
        "print()\n",
        "print(\"cifar100\")\n",
        "print(f\"train size: {len(cifar100_train_loader) * train_batch}\\ttest size: {len(cifar100_test_loader) * test_batch}\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist\n",
            "train size: 60000\ttest size: 10000\n",
            "\n",
            "cifar100\n",
            "train size: 50000\ttest size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8U_WoVFCQB0y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc_mnist = nn.Sequential(\n",
        "    nn.Linear(28*28, 16),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(16, 16),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(16, 10)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SY-cfhKlDQ2v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lenet5_mnist = createLenet5(in_channels=1, init_padding=(2,2), classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XViG_unSSpbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6f077bd3-0191-48c6-c26a-607e58a59aa5"
      },
      "cell_type": "code",
      "source": [
        "print(fc_mnist)\n",
        "print(lenet5_mnist)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=16, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (3): Sigmoid()\n",
            "  (4): Linear(in_features=16, out_features=10, bias=True)\n",
            ")\n",
            "Sequential(\n",
            "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hwZ5KP5cjC7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# t is of dims N * 1 where N is the batch size\n",
        "# C should be the number of values for the column\n",
        "def oneHotEncodeOneCol(t, C=2):\n",
        "    N = t.shape[0]\n",
        "    onehot = torch.Tensor([\n",
        "        [0] * C\n",
        "    ] * N)\n",
        "    for i, v in enumerate(t):\n",
        "        onehot[i, v] = 1\n",
        "    \n",
        "    return onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8DwZDTsg2FLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validate_every = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NSQaCLWglMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "c9406210-c60f-49ad-8c37-258d3103a580"
      },
      "cell_type": "code",
      "source": [
        "fc_mnist_dev = fc_mnist.to(device)\n",
        "opt = O.SGD(fc_mnist_dev.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_cross_entropy = []\n",
        "train_accuracy = []\n",
        "validation_cross_entropy = []\n",
        "validation_accuracy = []\n",
        "\n",
        "for epoch in range(2):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, batch in enumerate(mnist_train_loader):\n",
        "        x, labels = batch\n",
        "        x, labels = x.to(device), labels.to(device)\n",
        "        N = x.shape[0]\n",
        "        \n",
        "        x = x.view(N, -1)\n",
        "        \n",
        "        # training mode (for things like dropout)\n",
        "        fc_mnist_dev.train()\n",
        "        \n",
        "        # clear previous gradients\n",
        "        opt.zero_grad()\n",
        "        \n",
        "        y_hat = fc_mnist_dev(x)\n",
        "        loss = criterion(y_hat, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        train_cross_entropy.append(loss)\n",
        "        \n",
        "        n_correct += (torch.argmax(y_hat, dim=1) == labels).sum().item()\n",
        "        n_total += N\n",
        "        \n",
        "        # evaluation mode (e.g. adds dropped neurons back in)\n",
        "        fc_mnist_dev.eval()\n",
        "        if i % validate_every == 0:\n",
        "            n_val_correct = 0\n",
        "            n_val_total = 0\n",
        "            v_cross_entropy_sum = 0\n",
        "            \n",
        "            # don't calculate gradients here\n",
        "            with torch.no_grad():\n",
        "                for j, v_batch in enumerate(mnist_test_loader):\n",
        "                    v_x, v_labels = v_batch\n",
        "                    v_x, v_labels = v_x.to(device), v_labels.to(device)\n",
        "                    v_N = v_x.shape[0]\n",
        "                    v_x = v_x.view(v_N, -1)\n",
        "                    \n",
        "                    v_y_hat = fc_mnist_dev(v_x)\n",
        "                    v_loss = criterion(v_y_hat, v_labels)\n",
        "                    v_cross_entropy_sum += v_loss\n",
        "                    n_val_correct += (torch.argmax(v_y_hat, dim=1) == v_labels).sum().item()\n",
        "                    n_val_total += v_N\n",
        "\n",
        "            print(f\"[epoch {epoch + 1}, iteration {i}] \\t accuracy: {n_val_correct / n_val_total} \\t cross entropy: {v_cross_entropy_sum / n_val_total}\")\n",
        "            validation_accuracy.append(n_val_correct / n_val_total)\n",
        "            validation_cross_entropy.append(v_cross_entropy_sum / n_val_total)\n",
        "    \n",
        "    print(f\"epoch {epoch + 1} accumulated accuracy: {n_correct / n_total}\")\n",
        "    train_accuracy.append(n_correct / n_total)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 1, iteration 0] \t accuracy: 0.0892 \t cross_entropy: 0.002350575290620327\n",
            "[epoch 1, iteration 2000] \t accuracy: 0.0974 \t cross_entropy: 0.002298793289810419\n",
            "[epoch 1, iteration 4000] \t accuracy: 0.3534 \t cross_entropy: 0.0022665569558739662\n",
            "[epoch 1, iteration 6000] \t accuracy: 0.3388 \t cross_entropy: 0.0021684144157916307\n",
            "[epoch 1, iteration 8000] \t accuracy: 0.4672 \t cross_entropy: 0.0018440253334119916\n",
            "[epoch 1, iteration 10000] \t accuracy: 0.6009 \t cross_entropy: 0.001437817933037877\n",
            "[epoch 1, iteration 12000] \t accuracy: 0.6457 \t cross_entropy: 0.001186282024718821\n",
            "[epoch 1, iteration 14000] \t accuracy: 0.6523 \t cross_entropy: 0.0010444630170240998\n",
            "[epoch 1, iteration 16000] \t accuracy: 0.7225 \t cross_entropy: 0.0009443741291761398\n",
            "[epoch 1, iteration 18000] \t accuracy: 0.751 \t cross_entropy: 0.0008631504024378955\n",
            "[epoch 1, iteration 20000] \t accuracy: 0.7911 \t cross_entropy: 0.0007969893049448729\n",
            "[epoch 1, iteration 22000] \t accuracy: 0.7927 \t cross_entropy: 0.0007382668554782867\n",
            "[epoch 1, iteration 24000] \t accuracy: 0.8085 \t cross_entropy: 0.0006855062092654407\n",
            "[epoch 1, iteration 26000] \t accuracy: 0.8261 \t cross_entropy: 0.0006548158125951886\n",
            "[epoch 1, iteration 28000] \t accuracy: 0.8321 \t cross_entropy: 0.0006105313659645617\n",
            "[epoch 1, iteration 30000] \t accuracy: 0.859 \t cross_entropy: 0.0005644070915877819\n",
            "[epoch 1, iteration 32000] \t accuracy: 0.8594 \t cross_entropy: 0.0005399722722359002\n",
            "[epoch 1, iteration 34000] \t accuracy: 0.8616 \t cross_entropy: 0.0005178208812139928\n",
            "[epoch 1, iteration 36000] \t accuracy: 0.8621 \t cross_entropy: 0.0005172466044314206\n",
            "[epoch 1, iteration 38000] \t accuracy: 0.8715 \t cross_entropy: 0.0004789370868820697\n",
            "[epoch 1, iteration 40000] \t accuracy: 0.8743 \t cross_entropy: 0.0004700722056441009\n",
            "[epoch 1, iteration 42000] \t accuracy: 0.8841 \t cross_entropy: 0.00044129116577096283\n",
            "[epoch 1, iteration 44000] \t accuracy: 0.8781 \t cross_entropy: 0.00044018158223479986\n",
            "[epoch 1, iteration 46000] \t accuracy: 0.8789 \t cross_entropy: 0.00044080259976908565\n",
            "[epoch 1, iteration 48000] \t accuracy: 0.8902 \t cross_entropy: 0.00040975489537231624\n",
            "[epoch 1, iteration 50000] \t accuracy: 0.8947 \t cross_entropy: 0.00040107371751219034\n",
            "[epoch 1, iteration 52000] \t accuracy: 0.8933 \t cross_entropy: 0.00039334941538982093\n",
            "[epoch 1, iteration 54000] \t accuracy: 0.8948 \t cross_entropy: 0.00038483820389956236\n",
            "[epoch 1, iteration 56000] \t accuracy: 0.8988 \t cross_entropy: 0.0003686478012241423\n",
            "[epoch 1, iteration 58000] \t accuracy: 0.8972 \t cross_entropy: 0.00037704972783103585\n",
            "epoch 1 accuracy: 0.73565\n",
            "[epoch 2, iteration 0] \t accuracy: 0.9037 \t cross_entropy: 0.0003626386169344187\n",
            "[epoch 2, iteration 2000] \t accuracy: 0.902 \t cross_entropy: 0.0003685308329295367\n",
            "[epoch 2, iteration 4000] \t accuracy: 0.9073 \t cross_entropy: 0.0003451778320595622\n",
            "[epoch 2, iteration 6000] \t accuracy: 0.9078 \t cross_entropy: 0.00034016850986517966\n",
            "[epoch 2, iteration 8000] \t accuracy: 0.9096 \t cross_entropy: 0.00033155965502373874\n",
            "[epoch 2, iteration 10000] \t accuracy: 0.9029 \t cross_entropy: 0.0003399157430976629\n",
            "[epoch 2, iteration 12000] \t accuracy: 0.9117 \t cross_entropy: 0.00032307126093655825\n",
            "[epoch 2, iteration 14000] \t accuracy: 0.9019 \t cross_entropy: 0.0003436054685153067\n",
            "[epoch 2, iteration 16000] \t accuracy: 0.9122 \t cross_entropy: 0.00032436352921649814\n",
            "[epoch 2, iteration 18000] \t accuracy: 0.9144 \t cross_entropy: 0.0003048137587029487\n",
            "[epoch 2, iteration 20000] \t accuracy: 0.9123 \t cross_entropy: 0.00031248806044459343\n",
            "[epoch 2, iteration 22000] \t accuracy: 0.916 \t cross_entropy: 0.00030498934211209416\n",
            "[epoch 2, iteration 24000] \t accuracy: 0.9186 \t cross_entropy: 0.0002984164748340845\n",
            "[epoch 2, iteration 26000] \t accuracy: 0.9143 \t cross_entropy: 0.00029964675195515156\n",
            "[epoch 2, iteration 28000] \t accuracy: 0.916 \t cross_entropy: 0.00029262236785143614\n",
            "[epoch 2, iteration 30000] \t accuracy: 0.9196 \t cross_entropy: 0.00028449762612581253\n",
            "[epoch 2, iteration 32000] \t accuracy: 0.9206 \t cross_entropy: 0.0002903024142142385\n",
            "[epoch 2, iteration 34000] \t accuracy: 0.9203 \t cross_entropy: 0.0002854574704542756\n",
            "[epoch 2, iteration 36000] \t accuracy: 0.9198 \t cross_entropy: 0.00028117510373704135\n",
            "[epoch 2, iteration 38000] \t accuracy: 0.9241 \t cross_entropy: 0.0002699057222343981\n",
            "[epoch 2, iteration 40000] \t accuracy: 0.9218 \t cross_entropy: 0.0002748448168858886\n",
            "[epoch 2, iteration 42000] \t accuracy: 0.9218 \t cross_entropy: 0.00027501178556121886\n",
            "[epoch 2, iteration 44000] \t accuracy: 0.9165 \t cross_entropy: 0.000284796638879925\n",
            "[epoch 2, iteration 46000] \t accuracy: 0.9229 \t cross_entropy: 0.0002698331081774086\n",
            "[epoch 2, iteration 48000] \t accuracy: 0.9269 \t cross_entropy: 0.00026702540344558656\n",
            "[epoch 2, iteration 50000] \t accuracy: 0.9231 \t cross_entropy: 0.00026930077001452446\n",
            "[epoch 2, iteration 52000] \t accuracy: 0.9264 \t cross_entropy: 0.0002638560254126787\n",
            "[epoch 2, iteration 54000] \t accuracy: 0.9258 \t cross_entropy: 0.0002663350314833224\n",
            "[epoch 2, iteration 56000] \t accuracy: 0.9227 \t cross_entropy: 0.0002652661351021379\n",
            "[epoch 2, iteration 58000] \t accuracy: 0.9248 \t cross_entropy: 0.00026155990781262517\n",
            "epoch 2 accuracy: 0.9153666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nRqNhuM3AVif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc_train_cross_entropy = train_cross_entropy\n",
        "fc_train_accuracy = train_accuracy\n",
        "fc_validation_cross_entropy = validation_cross_entropy\n",
        "fc_validation_accuracy = validation_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SzRdOjC0_ySD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "ea045695-613f-4091-ad83-b900f6cd57ec"
      },
      "cell_type": "code",
      "source": [
        "lenet5_mnist_dev = lenet5_mnist.to(device)\n",
        "opt = O.SGD(lenet5_mnist_dev.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_cross_entropy = []\n",
        "train_accuracy = []\n",
        "validation_cross_entropy = []\n",
        "validation_accuracy = []\n",
        "\n",
        "for epoch in range(2):\n",
        "    n_correct = 0\n",
        "    n_total = 0\n",
        "    for i, batch in enumerate(mnist_train_loader):\n",
        "        x, labels = batch\n",
        "        x, labels = x.to(device), labels.to(device)\n",
        "        N = x.shape[0]\n",
        "        \n",
        "        # training mode (for things like dropout)\n",
        "        fc_mnist_dev.train()\n",
        "        \n",
        "        # clear previous gradients\n",
        "        opt.zero_grad()\n",
        "        \n",
        "        y_hat = lenet5_mnist_dev(x)\n",
        "        loss = criterion(y_hat, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        train_cross_entropy.append(loss)\n",
        "        \n",
        "        n_correct += (torch.argmax(y_hat, dim=1) == labels).sum().item()\n",
        "        n_total += N\n",
        "        \n",
        "        # evaluation mode (e.g. adds dropped neurons back in)\n",
        "        fc_mnist_dev.eval()\n",
        "        if i % validate_every == 0:\n",
        "            n_val_correct = 0\n",
        "            n_val_total = 0\n",
        "            v_cross_entropy_sum = 0\n",
        "            \n",
        "            # don't calculate gradients here\n",
        "            with torch.no_grad():\n",
        "                for j, v_batch in enumerate(mnist_test_loader):\n",
        "                    v_x, v_labels = v_batch\n",
        "                    v_x, v_labels = v_x.to(device), v_labels.to(device)\n",
        "                    v_N = v_x.shape[0]\n",
        "                    \n",
        "                    v_y_hat = lenet5_mnist_dev(v_x)\n",
        "                    v_loss = criterion(v_y_hat, v_labels)\n",
        "                    v_cross_entropy_sum += v_loss\n",
        "                    n_val_correct += (torch.argmax(v_y_hat, dim=1) == v_labels).sum().item()\n",
        "                    n_val_total += v_N\n",
        "\n",
        "            print(f\"[epoch {epoch + 1}, iteration {i}] \\t accuracy: {n_val_correct / n_val_total} \\t cross entropy: {v_cross_entropy_sum / n_val_total}\")\n",
        "            validation_accuracy.append(n_val_correct / n_val_total)\n",
        "            validation_cross_entropy.append(v_cross_entropy_sum / n_val_total)\n",
        "    \n",
        "    print(f\"epoch {epoch + 1} accumulated accuracy: {n_correct / n_total}\")\n",
        "    train_accuracy.append(n_correct / n_total)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 1, iteration 0] \t accuracy: 0.747 \t cross entropy: 0.0007258318364620209\n",
            "[epoch 1, iteration 2000] \t accuracy: 0.9043 \t cross entropy: 0.0003084908239543438\n",
            "[epoch 1, iteration 4000] \t accuracy: 0.9358 \t cross entropy: 0.00020822379156015813\n",
            "[epoch 1, iteration 6000] \t accuracy: 0.9527 \t cross entropy: 0.00015231870929710567\n",
            "[epoch 1, iteration 8000] \t accuracy: 0.9513 \t cross entropy: 0.00015651079593226314\n",
            "[epoch 1, iteration 10000] \t accuracy: 0.9625 \t cross entropy: 0.00012553349370136857\n",
            "[epoch 1, iteration 12000] \t accuracy: 0.963 \t cross entropy: 0.00012981842155568302\n",
            "[epoch 1, iteration 14000] \t accuracy: 0.9698 \t cross entropy: 9.72873458522372e-05\n",
            "[epoch 1, iteration 16000] \t accuracy: 0.9375 \t cross entropy: 0.0001960140943992883\n",
            "[epoch 1, iteration 18000] \t accuracy: 0.9475 \t cross entropy: 0.00016879368922673166\n",
            "[epoch 1, iteration 20000] \t accuracy: 0.9741 \t cross entropy: 8.303784125018865e-05\n",
            "[epoch 1, iteration 22000] \t accuracy: 0.9756 \t cross entropy: 7.968222053023055e-05\n",
            "[epoch 1, iteration 24000] \t accuracy: 0.9765 \t cross entropy: 8.151104702847078e-05\n",
            "[epoch 1, iteration 26000] \t accuracy: 0.966 \t cross entropy: 0.0001119805674534291\n",
            "[epoch 1, iteration 28000] \t accuracy: 0.9781 \t cross entropy: 6.951045361347497e-05\n",
            "[epoch 1, iteration 30000] \t accuracy: 0.9736 \t cross entropy: 8.444485138170421e-05\n",
            "[epoch 1, iteration 32000] \t accuracy: 0.9767 \t cross entropy: 7.709868077654392e-05\n",
            "[epoch 1, iteration 34000] \t accuracy: 0.9778 \t cross entropy: 7.213055505417287e-05\n",
            "[epoch 1, iteration 36000] \t accuracy: 0.9725 \t cross entropy: 8.976159733720124e-05\n",
            "[epoch 1, iteration 38000] \t accuracy: 0.9804 \t cross entropy: 6.0570626374101266e-05\n",
            "[epoch 1, iteration 40000] \t accuracy: 0.9819 \t cross entropy: 5.7970242778537795e-05\n",
            "[epoch 1, iteration 42000] \t accuracy: 0.9751 \t cross entropy: 8.520505070919171e-05\n",
            "[epoch 1, iteration 44000] \t accuracy: 0.9727 \t cross entropy: 8.383553358726203e-05\n",
            "[epoch 1, iteration 46000] \t accuracy: 0.9797 \t cross entropy: 6.28977722954005e-05\n",
            "[epoch 1, iteration 48000] \t accuracy: 0.9767 \t cross entropy: 7.20767566235736e-05\n",
            "[epoch 1, iteration 50000] \t accuracy: 0.9647 \t cross entropy: 0.00011180552974110469\n",
            "[epoch 1, iteration 52000] \t accuracy: 0.9747 \t cross entropy: 7.771504897391424e-05\n",
            "[epoch 1, iteration 54000] \t accuracy: 0.9756 \t cross entropy: 7.885046215960756e-05\n",
            "[epoch 1, iteration 56000] \t accuracy: 0.9783 \t cross entropy: 6.964871863601729e-05\n",
            "[epoch 1, iteration 58000] \t accuracy: 0.9803 \t cross entropy: 6.483463948825374e-05\n",
            "epoch 1 accumulated accuracy: 0.9614\n",
            "[epoch 2, iteration 0] \t accuracy: 0.9837 \t cross entropy: 5.321556091075763e-05\n",
            "[epoch 2, iteration 2000] \t accuracy: 0.9837 \t cross entropy: 5.576176044996828e-05\n",
            "[epoch 2, iteration 4000] \t accuracy: 0.9817 \t cross entropy: 6.059144652681425e-05\n",
            "[epoch 2, iteration 6000] \t accuracy: 0.9832 \t cross entropy: 5.420528395916335e-05\n",
            "[epoch 2, iteration 8000] \t accuracy: 0.9775 \t cross entropy: 7.83436989877373e-05\n",
            "[epoch 2, iteration 10000] \t accuracy: 0.9749 \t cross entropy: 8.161636651493609e-05\n",
            "[epoch 2, iteration 12000] \t accuracy: 0.9817 \t cross entropy: 5.846542990184389e-05\n",
            "[epoch 2, iteration 14000] \t accuracy: 0.9798 \t cross entropy: 6.757276423741132e-05\n",
            "[epoch 2, iteration 16000] \t accuracy: 0.9834 \t cross entropy: 5.142499503563158e-05\n",
            "[epoch 2, iteration 18000] \t accuracy: 0.9829 \t cross entropy: 6.165594822959974e-05\n",
            "[epoch 2, iteration 20000] \t accuracy: 0.9845 \t cross entropy: 5.325758320395835e-05\n",
            "[epoch 2, iteration 22000] \t accuracy: 0.985 \t cross entropy: 4.7955680201994255e-05\n",
            "[epoch 2, iteration 24000] \t accuracy: 0.9836 \t cross entropy: 5.4726355301681906e-05\n",
            "[epoch 2, iteration 26000] \t accuracy: 0.9835 \t cross entropy: 5.573169983108528e-05\n",
            "[epoch 2, iteration 28000] \t accuracy: 0.9817 \t cross entropy: 6.078036312828772e-05\n",
            "[epoch 2, iteration 30000] \t accuracy: 0.9835 \t cross entropy: 5.527635585167445e-05\n",
            "[epoch 2, iteration 32000] \t accuracy: 0.984 \t cross entropy: 5.290082117426209e-05\n",
            "[epoch 2, iteration 34000] \t accuracy: 0.9798 \t cross entropy: 6.916801066836342e-05\n",
            "[epoch 2, iteration 36000] \t accuracy: 0.9854 \t cross entropy: 4.8532896471442655e-05\n",
            "[epoch 2, iteration 38000] \t accuracy: 0.983 \t cross entropy: 5.8500925661064684e-05\n",
            "[epoch 2, iteration 40000] \t accuracy: 0.985 \t cross entropy: 4.812284169020131e-05\n",
            "[epoch 2, iteration 42000] \t accuracy: 0.9857 \t cross entropy: 4.95761887577828e-05\n",
            "[epoch 2, iteration 44000] \t accuracy: 0.984 \t cross entropy: 5.401574162533507e-05\n",
            "[epoch 2, iteration 46000] \t accuracy: 0.9872 \t cross entropy: 4.0621100197313353e-05\n",
            "[epoch 2, iteration 48000] \t accuracy: 0.9857 \t cross entropy: 4.54401524621062e-05\n",
            "[epoch 2, iteration 50000] \t accuracy: 0.9795 \t cross entropy: 6.998881872277707e-05\n",
            "[epoch 2, iteration 52000] \t accuracy: 0.9859 \t cross entropy: 4.72329520562198e-05\n",
            "[epoch 2, iteration 54000] \t accuracy: 0.9847 \t cross entropy: 4.596132203005254e-05\n",
            "[epoch 2, iteration 56000] \t accuracy: 0.9838 \t cross entropy: 4.7784844355192035e-05\n",
            "[epoch 2, iteration 58000] \t accuracy: 0.9739 \t cross entropy: 8.226872159866616e-05\n",
            "epoch 2 accumulated accuracy: 0.9811\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}